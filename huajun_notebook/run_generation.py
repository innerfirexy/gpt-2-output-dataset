import json
import nltk
import torch
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed

# model_name = "opt-125m"
# model = AutoModelForCausalLM.from_pretrained(f"facebook/{model_name}", torch_dtype=torch.float16).cuda()

# # the fast tokenizer currently does not work correctly
# tokenizer = AutoTokenizer.from_pretrained(f"facebook/{model_name}", use_fast=False)

# opt to gpt2
model_name = "gpt2"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).cuda()

# the fast tokenizer currently does not work correctly
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)

# get the prompt_text
prompt_list = []
opt_list = []
prompt_text = 'story_vary'
print("Reading prompt_text......")
with open(f"../zuhao/prompt_raw/{prompt_text}.txt", "r", encoding='utf-8') as file:
    content_lines = file.readlines()
prompt_list = [content_lines[i].rstrip('\n') for i in range(1, 15000, 3)]
for idx, prompt in tqdm(enumerate(prompt_list)):
    opt_list.append({"id": idx, "ended": False, "length": 0, "gen_text": ""})

# generate text using OPT models
print("Generating opt_text......")
for idx, prompt in tqdm(enumerate(prompt_list), total=len(prompt_list)):
    input_ids = tokenizer(prompt, truncation=True, max_length=1024, return_tensors="pt").input_ids.cuda()
    set_seed(32) # random seed
    generated_ids = model.generate(input_ids, do_sample=True, max_length=1024) # generated by top-k sampling
    gen_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    length = len(nltk.word_tokenize(gen_text))
    if gen_text[-1] == ".":
        opt_list[idx]["ended"] = True
    opt_list[idx]["length"] = length
    opt_list[idx]["gen_text"] = gen_text

# output generated text
print("Wrting opt_text......")
with open(f"../data/gpt2-generated-from-prompt/{model_name}-{prompt_text}.jsonl", "w") as file:
    for line in tqdm(opt_list, total=len(opt_list)):
        json.dump(line, file)
        file.write("\n")
