{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from SpectrumTools import *\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "human_list = [\n",
    "    '../data/gs_james/gs_news/webtext.train.model=.news_0.fft.csv',\n",
    "    '../data/gs_james/gs_news/webtext.train.model=.news_1.fft.csv',\n",
    "    '../data/gs_james/gs_news/webtext.train.model=.news_2.fft.csv',\n",
    "    '../data/gs_james/gs_news/webtext.train.model=.news_3.fft.csv',\n",
    "    '../data/gs_james/gs_news/webtext.train.model=.news_4.fft.csv',\n",
    "    '../data/gs_james/gs_story/webtext.train.model=.story_0.fft.csv',\n",
    "    '../data/gs_james/gs_story/webtext.train.model=.story_1.fft.csv',\n",
    "    '../data/gs_james/gs_story/webtext.train.model=.story_2.fft.csv',\n",
    "    '../data/gs_james/gs_story/webtext.train.model=.story_3.fft.csv',\n",
    "    '../data/gs_james/gs_story/webtext.train.model=.story_4.fft.csv',\n",
    "    '../data/gs_james/gs_wiki/webtext.train.model=.wiki_0.fft.csv',\n",
    "    '../data/gs_james/gs_wiki/webtext.train.model=.wiki_1.fft.csv',\n",
    "    '../data/gs_james/gs_wiki/webtext.train.model=.wiki_2.fft.csv',\n",
    "    '../data/gs_james/gs_wiki/webtext.train.model=.wiki_3.fft.csv',\n",
    "    '../data/gs_james/gs_wiki/webtext.train.model=.wiki_4.fft.csv',\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "human_list_old = [\n",
    "    '../data/data_gpt2_old/webtext.test.model=gpt2.fft.csv'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "gpt2_list = [\n",
    "    '../data/data_gpt2_old/small-117M.test.model=gpt2.fft.csv',\n",
    "    '../data/data_gpt2_old/medium-345M.test.model=gpt2.fft.csv',\n",
    "    '../data/data_gpt2_old/large-762M.test.model=gpt2.fft.csv',\n",
    "    '../data/data_gpt2_old/xl-1542M.test.model=gpt2.fft.csv'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compare gpt2 with webtext_old"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def alignPoints(filepath1: str, filepath2: str):\n",
    "    freq_list_list_1, power_list_list_1 = getInterval(filepath1)\n",
    "    freq_list_list_2, power_list_list_2 = getInterval(filepath2)\n",
    "    y1listlist, y2listlist = [], []\n",
    "    short_length = len(freq_list_list_1) if len(freq_list_list_1) < len(\n",
    "        freq_list_list_2) else len(freq_list_list_2)\n",
    "\n",
    "    for i in tqdm(range(short_length)):\n",
    "        freq_list1 = freq_list_list_1[i]\n",
    "        power_list1 = power_list_list_1[i]\n",
    "        freq_list2 = freq_list_list_2[i]\n",
    "        power_list2 = power_list_list_2[i]\n",
    "\n",
    "        func1 = getF(freq_list1, power_list1)\n",
    "        func2 = getF(freq_list2, power_list2)\n",
    "        # interpolate\n",
    "        x = np.linspace(0, 0.5, 1000)\n",
    "        y1 = func1(x)\n",
    "        y2 = func2(x)\n",
    "        y1listlist.append(y1)\n",
    "        y2listlist.append(y2)\n",
    "    return x, y1listlist, y2listlist"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "def getPSO(filepath1: str, filepath2: str):\n",
    "    area_floor_list, area_roof_list, pso_list = [], [], []\n",
    "    xlist, y1listlist, y2listlist = alignPoints(filepath1, filepath2)\n",
    "\n",
    "    for i in range(len(y1listlist)):\n",
    "        y1list = y1listlist[i]\n",
    "        y2list = y2listlist[i]\n",
    "        # Convert to non-negative values\n",
    "        y1list = [abs(i) for i in y1list]\n",
    "        y2list = [abs(i) for i in y2list]\n",
    "\n",
    "        ylists = []\n",
    "        ylists.append(y1list)\n",
    "        ylists.append(y2list)\n",
    "\n",
    "        y_intersection = np.amin(ylists, axis=0)\n",
    "        y_roof = np.amax(ylists, axis=0)\n",
    "        area_floor = np.trapz(y_intersection, xlist)\n",
    "        area_roof = np.trapz(y_roof, xlist)\n",
    "\n",
    "        area_floor_list.append(area_floor)\n",
    "        area_roof_list.append(area_roof)\n",
    "        pso_list.append(round(area_floor / area_roof, 4))\n",
    "\n",
    "    return area_floor_list, area_roof_list, pso_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "def getPearson(filepath1: str, filepath2: str):\n",
    "    xlist, y1listlist, y2listlist = alignPoints(filepath1, filepath2)\n",
    "    corr_list = []\n",
    "    for i in range(len(y1listlist)):\n",
    "        y1list = y1listlist[i]\n",
    "        y2list = y2listlist[i]\n",
    "        try:\n",
    "            corr, _ = pearsonr(y1list, y2list)\n",
    "        except ValueError:\n",
    "            # print('y1 len:', len(y1list), 'nan count:', len(y1list[np.isnan(y1list)]))\n",
    "            # print('y2 len:', len(y2list), 'nan count:', len(y2list[np.isnan(y2list)]))\n",
    "            y1list = np.nan_to_num(y1list)\n",
    "            y2list = np.nan_to_num(y2list)\n",
    "            corr, _ = pearsonr(y1list, y2list)\n",
    "        corr_list.append(corr)\n",
    "    return corr_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def getSpearmanr(filepath1: str, filepath2: str):\n",
    "    xlist, y1listlist, y2listlist = alignPoints(filepath1, filepath2)\n",
    "    corr_list = []\n",
    "    for i in range(len(y1listlist)):\n",
    "        y1list = y1listlist[i]\n",
    "        y2list = y2listlist[i]\n",
    "        corr, _ = spearmanr(y1list, y2list)\n",
    "        corr_list.append(corr)\n",
    "    return corr_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def getSAM(filepath1: str, filepath2: str):\n",
    "    xlist, y1listlist, y2listlist = alignPoints(filepath1, filepath2)\n",
    "    sam_list = []\n",
    "\n",
    "    for i in range(len(y1listlist)):\n",
    "        y1list = y1listlist[i]\n",
    "        y2list = y2listlist[i]\n",
    "        ylists = []\n",
    "        ylists.append(y1list)\n",
    "        ylists.append(y2list)\n",
    "\n",
    "        # Normalize the spectra\n",
    "        y1list /= np.linalg.norm(y1list)\n",
    "        y2list /= np.linalg.norm(y2list)\n",
    "        # Calculate the dot product\n",
    "        dot_product = np.dot(y1list, y2list)\n",
    "        # Calculate the SAM similarity\n",
    "        sam_similarity = np.arccos(dot_product) / np.pi\n",
    "        sam_list.append(sam_similarity)\n",
    "\n",
    "    return sam_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "# Record all the results\n",
    "results_dict = {'PSO': [], 'CORR': [], 'SAM': [], 'SPEAR': [], 'model': []}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4981/4981 [00:02<00:00, 1747.12it/s]\n",
      "100%|██████████| 4981/4981 [00:02<00:00, 2045.09it/s]\n",
      "100%|██████████| 4981/4981 [00:02<00:00, 2476.33it/s]\n",
      "100%|██████████| 4981/4981 [00:02<00:00, 2402.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webtext & gpt2-sm\n",
      "PSO: 0.3617340566606389 0.07221731235943037\n",
      "Corr: 0.6438794869424143 0.17855396411140867\n",
      "SAM: 0.2693253150087384 0.07215736359363699\n",
      "Spearmanr: 0.01318690335447418 0.06656590784301393\n",
      "PSO length: 4981\n",
      "Corr length: 4981\n",
      "SAM length: 4981\n",
      "Spearmanr length: 4981\n"
     ]
    }
   ],
   "source": [
    "filepath1 = human_list_old[0]\n",
    "filepath2 = gpt2_list[0] # gpt2-small\n",
    "\n",
    "area_floor_list, area_roof_list, pso_list = getPSO(filepath1, filepath2)\n",
    "corr_list = getPearson(filepath1, filepath2)\n",
    "sam_list = getSAM(filepath1, filepath2)\n",
    "spearmanr_list = getSpearmanr(filepath1, filepath2)\n",
    "\n",
    "print('Webtext & gpt2-sm')\n",
    "print('PSO:', np.nanmean(pso_list), np.nanstd(pso_list))\n",
    "print('Corr:', np.nanmean(corr_list), np.nanstd(corr_list))\n",
    "print('SAM:', np.nanmean(sam_list), np.nanstd(sam_list))\n",
    "print('Spearmanr:', np.nanmean(spearmanr_list), np.nanstd(spearmanr_list))\n",
    "\n",
    "print('PSO length:', len(pso_list))\n",
    "print('Corr length:', len(corr_list))\n",
    "print('SAM length:', len(sam_list))\n",
    "print('Spearmanr length:', len(spearmanr_list))\n",
    "\n",
    "results_dict['PSO'].append(pso_list)\n",
    "results_dict['CORR'].append(corr_list)\n",
    "results_dict['SAM'].append(sam_list)\n",
    "results_dict['SPEAR'].append(spearmanr_list)\n",
    "results_dict['model'].append(['gpt2-sm']*len(pso_list))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt2-sm', 'gpt2-sm', 'gpt2-sm', 'gpt2-sm', 'gpt2-sm', 'gpt2-sm', 'gpt2-sm', 'gpt2-sm', 'gpt2-sm', 'gpt2-sm']\n"
     ]
    }
   ],
   "source": [
    "# print(results_dict['model'][0][:10])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4994/4994 [00:02<00:00, 2261.44it/s]\n",
      "100%|██████████| 4994/4994 [00:02<00:00, 2304.14it/s]\n",
      "100%|██████████| 4994/4994 [00:02<00:00, 2126.15it/s]\n",
      "100%|██████████| 4994/4994 [00:02<00:00, 2128.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webtext & gpt2-md\n",
      "PSO: 0.35731927855711426 0.07198203901642747\n",
      "Corr: 0.6519632934082432 0.17578011743625077\n",
      "SAM: 0.26617845610364177 0.07231423620791073\n",
      "Spearmanr: 0.011838503218062336 0.06708215787518512\n",
      "PSO length: 4994\n",
      "Corr length: 4994\n",
      "SAM length: 4994\n",
      "Spearmanr length: 4994\n"
     ]
    }
   ],
   "source": [
    "filepath1 = human_list_old[0]\n",
    "filepath2 = gpt2_list[1] # gpt2-md\n",
    "\n",
    "area_floor_list, area_roof_list, pso_list = getPSO(filepath1, filepath2)\n",
    "corr_list = getPearson(filepath1, filepath2)\n",
    "sam_list = getSAM(filepath1, filepath2)\n",
    "spearmanr_list = getSpearmanr(filepath1, filepath2)\n",
    "\n",
    "print('Webtext & gpt2-md')\n",
    "print('PSO:', np.nanmean(pso_list), np.nanstd(pso_list))\n",
    "print('Corr:', np.nanmean(corr_list), np.nanstd(corr_list))\n",
    "print('SAM:', np.nanmean(sam_list), np.nanstd(sam_list))\n",
    "print('Spearmanr:', np.nanmean(spearmanr_list), np.nanstd(spearmanr_list))\n",
    "\n",
    "print('PSO length:', len(pso_list))\n",
    "print('Corr length:', len(corr_list))\n",
    "print('SAM length:', len(sam_list))\n",
    "print('Spearmanr length:', len(spearmanr_list))\n",
    "\n",
    "results_dict['PSO'].append(pso_list)\n",
    "results_dict['CORR'].append(corr_list)\n",
    "results_dict['SAM'].append(sam_list)\n",
    "results_dict['SPEAR'].append(spearmanr_list)\n",
    "results_dict['model'].append(['gpt2-md']*len(pso_list))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4994/4994 [00:02<00:00, 2269.40it/s]\n",
      "100%|██████████| 4994/4994 [00:01<00:00, 2579.79it/s]\n",
      "100%|██████████| 4994/4994 [00:02<00:00, 2495.34it/s]\n",
      "100%|██████████| 4994/4994 [00:02<00:00, 2418.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webtext & gpt2-lg\n",
      "PSO: 0.3664797514033681 0.06969912186815849\n",
      "Corr: 0.6389773676548709 0.1692137491025043\n",
      "SAM: 0.27212628021182306 0.06795641011453786\n",
      "Spearmanr: 0.014083957631672148 0.06632946210550243\n",
      "PSO length: 4994\n",
      "Corr length: 4994\n",
      "SAM length: 4994\n",
      "Spearmanr length: 4994\n"
     ]
    }
   ],
   "source": [
    "filepath1 = human_list_old[0]\n",
    "filepath2 = gpt2_list[2] # gpt2-lg\n",
    "\n",
    "area_floor_list, area_roof_list, pso_list = getPSO(filepath1, filepath2)\n",
    "corr_list = getPearson(filepath1, filepath2)\n",
    "sam_list = getSAM(filepath1, filepath2)\n",
    "spearmanr_list = getSpearmanr(filepath1, filepath2)\n",
    "\n",
    "print('Webtext & gpt2-lg')\n",
    "print('PSO:', np.nanmean(pso_list), np.nanstd(pso_list))\n",
    "print('Corr:', np.nanmean(corr_list), np.nanstd(corr_list))\n",
    "print('SAM:', np.nanmean(sam_list), np.nanstd(sam_list))\n",
    "print('Spearmanr:', np.nanmean(spearmanr_list), np.nanstd(spearmanr_list))\n",
    "\n",
    "print('PSO length:', len(pso_list))\n",
    "print('Corr length:', len(corr_list))\n",
    "print('SAM length:', len(sam_list))\n",
    "print('Spearmanr length:', len(spearmanr_list))\n",
    "\n",
    "results_dict['PSO'].append(pso_list)\n",
    "results_dict['CORR'].append(corr_list)\n",
    "results_dict['SAM'].append(sam_list)\n",
    "results_dict['SPEAR'].append(spearmanr_list)\n",
    "results_dict['model'].append(['gpt2-lg']*len(pso_list))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:02<00:00, 2414.64it/s]\n",
      "100%|██████████| 5000/5000 [00:02<00:00, 2214.81it/s]\n",
      "100%|██████████| 5000/5000 [00:02<00:00, 2414.98it/s]\n",
      "100%|██████████| 5000/5000 [00:02<00:00, 2432.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webtext & gpt2-xl\n",
      "PSO: 0.36732830000000005 0.06823007076876002\n",
      "Corr: 0.6389014870888231 0.1608899713174084\n",
      "SAM: 0.272738983726075 0.06495872443292001\n",
      "Spearmanr: 0.014658888762888764 0.06793662180989077\n",
      "PSO length: 5000\n",
      "Corr length: 5000\n",
      "SAM length: 5000\n",
      "Spearmanr length: 5000\n"
     ]
    }
   ],
   "source": [
    "filepath1 = human_list_old[0]\n",
    "filepath2 = gpt2_list[3] # gpt2-xl\n",
    "\n",
    "area_floor_list, area_roof_list, pso_list = getPSO(filepath1, filepath2)\n",
    "corr_list = getPearson(filepath1, filepath2)\n",
    "sam_list = getSAM(filepath1, filepath2)\n",
    "spearmanr_list = getSpearmanr(filepath1, filepath2)\n",
    "\n",
    "print('Webtext & gpt2-xl')\n",
    "print('PSO:', np.nanmean(pso_list), np.nanstd(pso_list))\n",
    "print('Corr:', np.nanmean(corr_list), np.nanstd(corr_list))\n",
    "print('SAM:', np.nanmean(sam_list), np.nanstd(sam_list))\n",
    "print('Spearmanr:', np.nanmean(spearmanr_list), np.nanstd(spearmanr_list))\n",
    "\n",
    "print('PSO length:', len(pso_list))\n",
    "print('Corr length:', len(corr_list))\n",
    "print('SAM length:', len(sam_list))\n",
    "print('Spearmanr length:', len(spearmanr_list))\n",
    "\n",
    "results_dict['PSO'].append(pso_list)\n",
    "results_dict['CORR'].append(corr_list)\n",
    "results_dict['SAM'].append(sam_list)\n",
    "results_dict['SPEAR'].append(spearmanr_list)\n",
    "results_dict['model'].append(['gpt2-xl']*len(pso_list))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import chain\n",
    "\n",
    "for key, val in results_dict.items():\n",
    "    tmp = list(chain.from_iterable(val))\n",
    "    results_dict[key] = tmp\n",
    "\n",
    "df = pd.DataFrame(results_dict)\n",
    "df.to_csv('FACE_GPT2_old.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
